# Day 25: LLM Internalsâ€”Attention, Transformers, and Scaling Laws

## Overview
Day 25 dives into the internals of LLMs: attention, transformer architecture, and scaling laws.

## What You'll Learn
- How attention works in transformers
- Why transformers
- The impact of scaling laws on model performance

## Demo
The Python script `attention_viz.py` demonstrates:
- Visualizing attention weights in a transformer model

## How to Run
1. Install dependencies:
   ```bash
   pip install transformers torch matplotlib
Run the script:-
python code/attention_viz.py
