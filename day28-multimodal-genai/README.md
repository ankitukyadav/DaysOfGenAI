# Day 28: Multimodal GenAIâ€”Text + Images with CLIP & BLIP

## Overview
Day 28 explores multimodal GenAI: connecting text and images with models like CLIP and BLIP.

## What You'll Learn
- How multimodal models work
- How to use CLIP for image-text matching
- Why vision-language models matter

## Demo
The Python script `clip_demo.py` demonstrates:
- Matching images to text queries using CLIP

## How to Run
1. Install dependencies:
   ```bash
   pip install torch clip-by-openai pillow
2. Add sample images (cat.jpg, dog.jpg) to the folder.
3. Run the script:
python code/clip_demo.py
